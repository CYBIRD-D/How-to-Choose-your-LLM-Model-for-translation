这里以LM STUDIO为例
<p align="center">
  <img src="./LM%20STUDIO.png" width="800" alt="LM Studio 截图">
</p>

### **Context Length（上下文长度）** </br>
上下文长度是指模型在一次前向/解码时可同时“看见”的最大输入序列（输入+中间缓存+已生成历史）的 **令牌数（tokens）** 上限。 

令牌（token）≠字/词。英文经验换算：1 token ≈ 4 个字符 ≈ 0.75 个词（不同语言不同）。超出上限会被截断或报错。
- 数值越大，能处理的长文本越多，但显著增加显存/内存与计算开销，超过模型训练或实现允许的最大窗口还可能导致输出异常。

- 模型参数越大，context（上下文）所需的显存/内存越多；上下文窗口越长，所需的显存/内存越多 </br>
-----

### **GPU Offload（GPU 分层卸载）** </br>
- 指定将多少层网络权重与计算卸载到 GPU 上执行（llama.cpp 对应 n_gpu_layers/-ngl；≥所有层数通常意味着尽量全卸载到GPU）。

- 更多层在 GPU 上可显著提升吞吐，但受显存限制；显存不足部分仍会在 CPU 上跑
------

### **CPU Thread Pool Size（CPU 线程池大小）** </br>
LM Studio 在推理时分配给模型计算的 CPU 工作线程数。在 SDK / API 里对应 cpuThreads 字段（推理期参数，非加载期参数）
- **CPU-only(仅用CPU) 推理**：线程数越多，越能并行 GGML/llama.cpp 的算子计算；但很快会受 内存带宽 限制，继续增加线程数收益递减甚至变慢。

- **GPU Offload 场景（Vulkan / CUDA）**：大量权重在 GPU 上算，但仍有一部分计算与采样在 CPU 侧执行，cpuThreads 依旧起作用；通常此时 CPU 不再是唯一瓶颈，线程调太高的收益会更小(社区报告甚至存在负收益）
> 具体设置数量请以**物理核心数**（不是逻辑线程数/超线程总数），通常接近物理核数或系统建议值即可
------

### **Evaluation Batch Size（评估批大小 / n_batch）** </br>
**定义**：Evaluation batch size / evalBatchSize 指一次前向计算中并行送入模型、一起计算的输入 token 数。
- 把它调大，通常能更好地利用并行计算，从而提升**吞吐量/预填充（prefill）速度**；但代价是占用更多显/内存；结果与语义不会因批大小改变，只影响速度与资源占用。

- **与 llama.cpp 的关系**：LM Studio 基于 llama.cpp。llama.cpp 里常见到的 n_batch（逻辑批大小）≈ 这里的 evalBatchSize；底层还会按 **n_ubatch（微批）**再切分以适配显存/内存，因此即使把 evalBatchSize 拉很高，底层也可能分块跑。所以会出 **调大后不一定线性提**的现象。
> 从保守值起步（多数机器 512 是安全起点）。观察 tokens/s（生成速度）与交互流畅度，逐步翻倍试探（512→1024→2048…）

--------

### **RoPE Frequency Base（RoPE 频率基数）**  </br>
调整旋转位置编码（RoPE）的基频。该进阶参数影响模型如何编码位置信息；适当调大常用于尝试在更长上下文下保持稳定性（具体极限取决于模型/实现）。
- **不了解不要动此选项!**
-------

### **RoPE Frequency Scale（RoPE 频率缩放）** </br>
RoPE 的缩放因子；改变位置编码的“粒度”，常与上项配合，用于扩展有效上下文或做长上下文实验。
- **不了解不要动此选项!**
-------

### **Offload KV Cache to GPU Memory（将 KV 缓存卸载到 GPU）** </br>
让 K/V 缓存常驻 GPU 显存（而不是放在系统内存/CPU），这样注意力读取 K/V 时无需跨 PCIe 往返，通常可降低 CPU/RAM 压力并显著提升上下文的速度，但会占用更多显存
- LM Studio 在 v0.3.9 更新里把这个选项做成了模型加载选项与 GPU 设置面板中的显式开关
------

### **Keep Model in Memory（将模型常驻内存）** </br>
使已加载的模型不被自动移出系统内存，换取**更快的再次调用与交互体验**；代价是占用更多 RAM。

---------
### **Try mmap()（尝试内存映射）** </br>
通过内存映射从磁盘“按需”映射权重，通常能**加快加载并减少常驻内存占用**；但当模型大于可用 RAM 时可能产生频繁页换而降速。
- LM Studio 提供该开关；llama.cpp 默认使用 mmap，可按需要禁用。

--------
### **Seed（随机种子）** </br>
控制采样中的随机性以便复现实验结果；设定固定值可让同一提示在同参数下产生一致输出。

--------
### **Flash Attention（实验性/可选）** </br>
**定义**：Flash Attention（FA）是一种I/O 感知的、精确（exact）的注意力实现。它通过在 GPU 上将 Q·Kᵀ→softmax→·V 分块/tiling，把中间结果留在片上 SRAM，极大减少 HBM（显存）读写与中间激活占用，从而在长序列与高吞吐时显著加速，同时不改变结果（非近似）
- 官方文档说:**启用可减少内存并加速生成，尤其长序**

- FA 已在 CUDA 与 Metal 后端可用；社区讨论也明确这一点（SYCL/ROCm 仍在推进/不完整）。因此，NVIDIA/Mac(M系列）用户最易受益。
------
### **K Cache Quantization Type（K 缓存量化类型，实验性）** </br>
选择注意力中 K（Key）缓存的存储精度/量化格式（对应实现里的 type_k/ggml 类型）。量化 KV 缓存可明显降低内存/显存占用以换取极小精度损失或在部分场景带来速度收益；可用格式与稳定性依实现与硬件而异。
- 因模型量化，**不了解请不要动此选项**
-------
### **V Cache Quantization Type（V 缓存量化类型，实验性）** </br>
同上，但作用于 V（Value）缓存（实现字段 type_v）。与 K 缓存配合量化常用于长上下文或小显存卡以扩大可用窗口/降低占用。
- 因模型量化，**不了解请不要动此选项**
---------
### Speculative Decoding（推测/投机解码）[LM STUDIO官方介绍](https://lmstudio.ai/blog/lmstudio-v0.3.10)  
在 LM Studio 里，Speculative Decoding（推测/投机解码）是一种双模型协同的推理加速：用一个更小更快的**草稿模型**先并行起草一串候选 token，再让更大的**主模型**快速验证并只接受那些与它本来会生成的结果一致的 token，从而在**不牺牲输出分布/质量的前提下提升生成速度**。
- 草稿模型一次性预测接下来若干 token；主模型并行计算这些位置的分布，匹配则整段接受，否则回退到第一处不匹配点并继续常规解码；主模型在接受最后一个草稿 token 后还会额外再出 1 个 token以保证分布无偏。
  - **兼容性**：草稿模型需与主模型共享足够一致的词表/分词器（同一“vocabulary”），LM Studio 会自动检查并在 UI 中提示可用配对。
  
- **配对建议**：优先用同一模型家族的小版作草稿，例如：  
Llama 3.1 8B ↔ Llama 3.2 1B；Qwen 2.5 14B ↔ Qwen 2.5 0.5B  

- 大小比例（LM Studio 文档给出的指引）

| 主模型大小 | 推荐草稿模型大小 |
|---|---:|
| 3–7B | ≤ 1B |
| 14B | ≤ 3B |
| 32B | ≤ 7B |
| | **命中率可接受 </br>草稿越小越好** |

- **API 使用**：本地服务器同样支持；在服务器 UI 中设置草稿模型；或在请求里加入 draft_model 字段
> 速度为何“有时很快、有时一般”
> 1. 草稿模型相对越小越快；
> 2. 草稿模型对当前提示的 **建议命中率（acceptance rate）** 要高。草稿越小、命中越高，提速越明显。
> 3. **提示依赖性（prompt dependent）** ：事实性、结构性强的问题（如公式/代码）更容易被大模型接受整段草稿；而开放创意写作分支多，拒绝更频繁、提速变小。
> 4. **资源权衡**：同时跑两套模型会增加显存/内存与计算占用；若草稿过大或命中率低，甚至会变慢。
