这里以LM STUDIO为例
<p align="center">
  <img src="./LM%20STUDIO.png" width="800" alt="LM Studio 截图">
</p>

### **Context Length（上下文长度）** </br>
上下文长度是指模型在一次前向/解码时可同时“看见”的最大输入序列（输入+中间缓存+已生成历史）的 **令牌数（tokens）** 上限。 

令牌（token）≠字/词。英文经验换算：1 token ≈ 4 个字符 ≈ 0.75 个词（不同语言不同）。超出上限会被截断或报错。
- 数值越大，能处理的长文本越多，但显著增加显存/内存与计算开销，超过模型训练或实现允许的最大窗口还可能导致输出异常。
- 模型参数越大，context（上下文）所需的显存/内存越多；上下文窗口越长，所需的显存/内存越多 </br>
-----

### **GPU Offload（GPU 分层卸载）** </br>
- 指定将多少层网络权重与计算卸载到 GPU 上执行（llama.cpp 对应 n_gpu_layers/-ngl；设为 -1 或 ≥所有层数通常意味着尽量全卸载到GPU）。
- 更多层在 GPU 上可显著提升吞吐，但受显存限制；显存不足部分仍会在 CPU 上跑
------

### **CPU Thread Pool Size（CPU 线程池大小）** </br>
LM Studio 在推理时分配给模型计算的 CPU 工作线程数。在 SDK / API 里对应 cpuThreads 字段（推理期参数，非加载期参数）
- **CPU-only(仅用CPU) 推理**：线程数越多，越能并行 GGML/llama.cpp 的算子计算；但很快会受 内存带宽 限制，继续增加线程数收益递减甚至变慢。
- **GPU Offload 场景（Vulkan / CUDA）**：大量权重在 GPU 上算，但仍有一部分计算与采样在 CPU 侧执行，cpuThreads 依旧起作用；通常此时 CPU 不再是唯一瓶颈，线程调太高的收益会更小(社区报告甚至存在负收益）
- 具体设置数量请以**物理核心数**（不是逻辑线程数/超线程总数），通常接近物理核数或系统建议值即可
------

### **Evaluation Batch Size（评估批大小 / n_batch）** </br>
**定义**：Evaluation batch size / evalBatchSize 指一次前向计算中并行送入模型、一起计算的输入 token 数。
- 把它调大，通常能更好地利用并行计算，从而提升“吞吐量/预填充（prefill）速度”；但代价是占用更多显/内存；结果与语义不会因批大小改变，只影响速度与资源占用。
- **与 llama.cpp 的关系**：LM Studio 基于 llama.cpp。llama.cpp 里常见到的 n_batch（逻辑批大小）≈ 这里的 evalBatchSize；底层还会按 **n_ubatch（微批）**再切分以适配显存/内存，因此即使把 evalBatchSize 拉很高，底层也可能分块跑。所以会出现“调大后不一定线性提速”的现象。
- 从保守值起步（多数机器 512 是安全起点）。观察 tokens/s（生成速度）与交互流畅度，逐步翻倍试探（512→1024→2048…）

--------

### **RoPE Frequency Base（RoPE 频率基数）**  </br>
调整旋转位置编码（RoPE）的基频。该进阶参数影响模型如何编码位置信息；适当调大常用于尝试在更长上下文下保持稳定性（具体极限取决于模型/实现）。
- 不了解不要动此选项
-------

### **RoPE Frequency Scale（RoPE 频率缩放）** </br>
RoPE 的缩放因子；改变位置编码的“粒度”，常与上项配合，用于扩展有效上下文或做长上下文实验。
- 不了解不要动此选项
-------

### **Offload KV Cache to GPU Memory（将 KV 缓存卸载到 GPU）** </br>
让 K/V 缓存常驻 GPU 显存（而不是放在系统内存/CPU），这样注意力读取 K/V 时无需跨 PCIe 往返，通常可降低 CPU/RAM 压力并显著提升上下文的速度，但会占用更多显存
- LM Studio 在 v0.3.9 更新里把这个选项做成了模型加载选项与 GPU 设置面板中的显式开关
------

### **Keep Model in Memory（将模型常驻内存）** </br>
使已加载的模型不被自动移出系统内存，换取更快的再次调用与交互体验；代价是占用更多 RAM。

---------
### **Try mmap()（尝试内存映射）** </br>
通过内存映射从磁盘“按需”映射权重，通常能加快加载并减少常驻内存占用；但当模型大于可用 RAM 时可能产生频繁页换而降速。
- LM Studio 提供该开关；llama.cpp 默认使用 mmap，可按需要禁用。

--------
### **Seed（随机种子）** </br>
控制采样中的随机性以便复现实验结果；设定固定值可让同一提示在同参数下产生一致输出。

--------
### **Flash Attention（实验性/可选）** </br>
**定义**：Flash Attention（FA）是一种I/O 感知的、精确（exact）的注意力实现。它通过在 GPU 上将 Q·Kᵀ→softmax→·V 分块/tiling，把中间结果留在片上 SRAM，极大减少 HBM（显存）读写与中间激活占用，从而在长序列与高吞吐时显著加速，同时不改变结果（非近似）
- 官方文档说明“启用可减少内存并加速生成，尤其长序列”
- FA 已在 CUDA 与 Metal 后端可用；社区讨论也明确这一点（SYCL/ROCm 仍在推进/不完整）。因此，NVIDIA/Mac(M系列）用户最易受益。
------
### **K Cache Quantization Type（K 缓存量化类型，实验性）** </br>
选择注意力中 K（Key）缓存的存储精度/量化格式（对应实现里的 type_k/ggml 类型）。量化 KV 缓存可明显降低内存/显存占用以换取极小精度损失或在部分场景带来速度收益；可用格式与稳定性依实现与硬件而异。
- 因模型量化，不了解请不要动此选项
-------
### **V Cache Quantization Type（V 缓存量化类型，实验性）** </br>
同上，但作用于 V（Value）缓存（实现字段 type_v）。与 K 缓存配合量化常用于长上下文或小显存卡以扩大可用窗口/降低占用。
- 因模型量化，不了解请不要动此选项
